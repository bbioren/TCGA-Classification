{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef1f5700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01432105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data shape: (658, 788)\n",
      "      emb_0     emb_1     emb_2     emb_3     emb_4     emb_5     emb_6  \\\n",
      "0 -0.372847  1.175885 -0.136382  0.584974 -0.075458  0.242736 -0.091984   \n",
      "1 -0.356913  0.706686  0.037918  0.460797 -0.655461 -0.034329 -0.260771   \n",
      "2 -0.650745  0.622192 -0.541576 -0.147439 -0.688726  0.005337  0.160113   \n",
      "3 -0.243454  0.907251 -0.278341  0.578471 -0.371568  0.370125 -0.096557   \n",
      "4 -0.501727  0.816661 -0.331489  0.102610 -0.754066  0.199273  0.021237   \n",
      "\n",
      "      emb_7     emb_8     emb_9  ...  diagnoses.laterality  \\\n",
      "0  0.850612  1.272928 -0.353911  ...                  Left   \n",
      "1  0.986547  1.546444 -0.446621  ...                 Right   \n",
      "2  0.956877  0.844839 -0.026274  ...                 Right   \n",
      "3  0.984593  1.147140 -0.293077  ...                 Right   \n",
      "4  0.814512  1.281691 -0.193750  ...                 Right   \n",
      "\n",
      "   diagnoses.morphology  diagnoses.prior_malignancy  \\\n",
      "0                8140/3                         yes   \n",
      "1                8140/3                          no   \n",
      "2                8140/3                          no   \n",
      "3                8255/3                          no   \n",
      "4                8255/3                          no   \n",
      "\n",
      "   diagnoses.residual_disease  diagnoses.tissue_or_organ_of_origin  \\\n",
      "0                          R2                     Upper lobe, lung   \n",
      "1                          R0                     Lower lobe, lung   \n",
      "2                          R2                     Lower lobe, lung   \n",
      "3                          R0                     Upper lobe, lung   \n",
      "4                          R0                     Upper lobe, lung   \n",
      "\n",
      "   treatments.therapeutic_agents  \\\n",
      "0                            NaN   \n",
      "1                            NaN   \n",
      "2                            NaN   \n",
      "3                            NaN   \n",
      "4   ['Cisplatin', 'Vinorelbine']   \n",
      "\n",
      "                           treatments.treatment_type  \\\n",
      "0  ['Radiation Therapy, NOS', 'Pharmaceutical The...   \n",
      "1  ['Radiation Therapy, NOS', 'Pharmaceutical The...   \n",
      "2                                                NaN   \n",
      "3  ['Radiation Therapy, NOS', 'Pharmaceutical The...   \n",
      "4         ['Chemotherapy', 'Radiation Therapy, NOS']   \n",
      "\n",
      "   exposures.pack_years_smoked           exposures.tobacco_smoking_status  OS  \n",
      "0                         32.0  Current Reformed Smoker for < or = 15 yrs   1  \n",
      "1                         52.0       Current Reformed Smoker for > 15 yrs   1  \n",
      "2                         47.0  Current Reformed Smoker for < or = 15 yrs   0  \n",
      "3                         43.0       Current Reformed Smoker for > 15 yrs   1  \n",
      "4                         15.0  Current Reformed Smoker for < or = 15 yrs   1  \n",
      "\n",
      "[5 rows x 788 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 658 entries, 0 to 657\n",
      "Columns: 788 entries, emb_0 to OS\n",
      "dtypes: float64(770), int64(1), object(17)\n",
      "memory usage: 4.0+ MB\n",
      "None\n",
      "OS\n",
      "1    439\n",
      "0    219\n",
      "Name: count, dtype: int64\n",
      "(658, 788)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings_df = pd.read_csv(\"out/embeddings_processed.csv\")\n",
    "structured_df = pd.read_csv(\"out/data.csv\")\n",
    "\n",
    "embeddings_df = embeddings_df['embedding'].str.split(',', expand=True)\n",
    "\n",
    "# convert all columns to float\n",
    "embeddings_df = embeddings_df.astype(float)\n",
    "\n",
    "# rename columns like 'emb_0', 'emb_1', ...\n",
    "embeddings_df.columns = [f'emb_{i}' for i in range(embeddings_df.shape[1])]\n",
    "\n",
    "data = pd.concat([embeddings_df, structured_df], axis=1)\n",
    "\n",
    "data = data.drop(columns=[\"cases.submitter_id\"])\n",
    "\n",
    "# Step 3: Initial exploration\n",
    "print(\"Initial data shape:\", data.shape)\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "print(data['OS'].value_counts())\n",
    "\n",
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52d6bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    if data[col].dtype == 'object':\n",
    "        data[col] = data[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e18ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = data.select_dtypes(include=[np.number]).columns\n",
    "cat_cols = data.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50a8600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imputer = SimpleImputer(strategy='median')\n",
    "for col in num_cols:\n",
    "    if pd.api.types.is_numeric_dtype(data[col]):\n",
    "        data[col] = data[col].astype(float)\n",
    "        data[col] = num_imputer.fit_transform(data[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bfc1fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(cat_cols) > 0:\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    data[cat_cols] = pd.DataFrame(\n",
    "        cat_imputer.fit_transform(data[cat_cols]),\n",
    "        columns=cat_cols,\n",
    "        index=data.index\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c2b2927",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_dict = {}\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    le_dict[col] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8ab4d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embedding features: 768\n",
      "Number of structured data features: 19\n",
      "New total feature count: 39 (20 PCA components + 19 scaled features)\n"
     ]
    }
   ],
   "source": [
    "X = data.drop('OS', axis=1)\n",
    "y = data['OS']\n",
    "\n",
    "# --- 1. Identify feature groups ---\n",
    "# All columns starting with 'emb_' are embedding features\n",
    "embedding_cols = [col for col in X.columns if col.startswith('emb_')]\n",
    "# All other columns are structured data features (now label-encoded and imputed)\n",
    "data_cols = [col for col in X.columns if not col.startswith('emb_')]\n",
    "\n",
    "print(f\"Number of embedding features: {len(embedding_cols)}\")\n",
    "print(f\"Number of structured data features: {len(data_cols)}\")\n",
    "\n",
    "# --- 2. Define Pipelines ---\n",
    "\n",
    "# Pipeline 1: For Embeddings (Scale -> PCA)\n",
    "pca_pipeline = Pipeline(steps=[\n",
    "    # It is essential to scale the data before applying PCA\n",
    "    ('scaler', StandardScaler()),\n",
    "    # Reduce the embeddings to 20 components\n",
    "    ('pca', PCA(n_components=20))\n",
    "])\n",
    "\n",
    "# Pipeline 2: For Structured Data (Only Scale)\n",
    "# We scale the structured data features to ensure they are on the same magnitude\n",
    "# as the new PCA components, improving model performance.\n",
    "structured_pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "\n",
    "# --- 3. Create the ColumnTransformer (Combined Data) ---\n",
    "preprocessor_combined = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Apply the PCA pipeline to the embedding columns\n",
    "        ('pca_transform', pca_pipeline, embedding_cols),\n",
    "        # Apply the scaling pipeline to the structured data columns\n",
    "        ('structured_scale', structured_pipeline, data_cols)\n",
    "    ],\n",
    "    remainder='drop', # Drop any columns not explicitly named\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# --- 4. Apply the Transformation (Combined Data) ---\n",
    "X_combined = preprocessor_combined.fit_transform(X)\n",
    "\n",
    "print(f\"New total feature count: {X_combined.shape[1]} (20 PCA components + {len(data_cols)} scaled features)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "new_cell_structured",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SCENARIO 1: Structured Data Only ---\n",
      "Total features: 19\n",
      "Mean CV Score (Structured Only): 0.6964\n",
      "Std Dev CV Score: 0.0446\n"
     ]
    }
   ],
   "source": [
    "# === SCENARIO 1: JUST STRUCTURED DATA ===\n",
    "\n",
    "# 1. Create a ColumnTransformer that isolates structured data\n",
    "preprocessor_structured = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Apply scaling to structured features\n",
    "        ('structured_scale', structured_pipeline, data_cols),\n",
    "        # DROP embedding features\n",
    "        ('drop_embeddings', 'drop', embedding_cols)\n",
    "    ],\n",
    "    remainder='drop', \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 2. Apply the Transformation\n",
    "X_structured_only = preprocessor_structured.fit_transform(X)\n",
    "\n",
    "print(\"\\n--- SCENARIO 1: Structured Data Only ---\")\n",
    "print(f\"Total features: {X_structured_only.shape[1]}\")\n",
    "\n",
    "# 3. Run Cross-Validation on Structured Data Only\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    random_state=42, \n",
    "    class_weight='balanced', \n",
    "    n_jobs=-1\n",
    ")\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = 'roc_auc'\n",
    "\n",
    "cv_scores_structured = cross_val_score(\n",
    "    estimator=model, \n",
    "    X=X_structured_only, \n",
    "    y=y, \n",
    "    cv=skf,             \n",
    "    scoring=scoring,    \n",
    "    n_jobs=-1           \n",
    ")\n",
    "\n",
    "print(f\"Mean CV Score (Structured Only): {cv_scores_structured.mean():.4f}\")\n",
    "print(f\"Std Dev CV Score: {cv_scores_structured.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "new_cell_embedding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SCENARIO 2: Embedding Data Only ---\n",
      "Total features: 20\n",
      "Mean CV Score (Embedding Only): 0.5214\n",
      "Std Dev CV Score: 0.0195\n"
     ]
    }
   ],
   "source": [
    "# === SCENARIO 2: JUST EMBEDDING DATA ===\n",
    "\n",
    "# 1. Create a ColumnTransformer that isolates embedding data (with PCA)\n",
    "preprocessor_embedding = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Apply scaling and PCA to embedding features\n",
    "        ('pca_transform', pca_pipeline, embedding_cols),\n",
    "        # DROP structured features\n",
    "        ('drop_structured', 'drop', data_cols)\n",
    "    ],\n",
    "    remainder='drop', \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 2. Apply the Transformation\n",
    "X_embedding_only = preprocessor_embedding.fit_transform(X)\n",
    "\n",
    "print(\"\\n--- SCENARIO 2: Embedding Data Only ---\")\n",
    "print(f\"Total features: {X_embedding_only.shape[1]}\")\n",
    "\n",
    "# 3. Run Cross-Validation on Embedding Data Only\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    random_state=42, \n",
    "    class_weight='balanced', \n",
    "    n_jobs=-1\n",
    ")\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = 'roc_auc'\n",
    "\n",
    "cv_scores_embedding = cross_val_score(\n",
    "    estimator=model, \n",
    "    X=X_embedding_only, \n",
    "    y=y, \n",
    "    cv=skf,             \n",
    "    scoring=scoring,    \n",
    "    n_jobs=-1           \n",
    ")\n",
    "\n",
    "print(f\"Mean CV Score (Embedding Only): {cv_scores_embedding.mean():.4f}\")\n",
    "print(f\"Std Dev CV Score: {cv_scores_embedding.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9c21b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stratified 5-Fold Cross-Validation...\n",
      "\n",
      "Stratified 5-Fold Cross-Validation (roc_auc Scores):\n",
      "[0.67716942 0.63106921 0.66567665 0.65050211 0.73471787]\n",
      "\n",
      "Mean CV Score (Combined Data): 0.6718\n",
      "Standard Deviation of CV Score: 0.0350\n"
     ]
    }
   ],
   "source": [
    "# === SCENARIO 3: COMBINED DATA (Baseline) ===\n",
    "# This cell uses the X_combined variable calculated in cell 21.\n",
    "\n",
    "# 1. Define the Model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    random_state=42, \n",
    "    class_weight='balanced', \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 2. Define the Cross-Validation Strategy\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=5,         \n",
    "    shuffle=True,       \n",
    "    random_state=42     \n",
    ")\n",
    "\n",
    "# 3. Define the Metric\n",
    "scoring = 'roc_auc'\n",
    "\n",
    "# 4. Perform Stratified Cross-Validation\n",
    "print(f\"Starting Stratified {skf.n_splits}-Fold Cross-Validation...\")\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    estimator=model, \n",
    "    X=X_combined, \n",
    "    y=y, \n",
    "    cv=skf,             \n",
    "    scoring=scoring,    \n",
    "    n_jobs=-1           \n",
    ")\n",
    "\n",
    "# 5. Output the Results\n",
    "print(f\"\\nStratified {skf.n_splits}-Fold Cross-Validation ({scoring} Scores):\")\n",
    "print(cv_scores)\n",
    "print(f\"\\nMean CV Score (Combined Data): {cv_scores.mean():.4f}\")\n",
    "print(f\"Standard Deviation of CV Score: {cv_scores.std():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
